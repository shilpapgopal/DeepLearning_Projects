{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3218\n",
      "vocab len- 3222\n",
      "5015\n",
      "79\n",
      "5015\n",
      "-----------BEGIN BLEU SCORING----------------\n",
      "405534893_2d0f3b0147 ---HIGH BLEU SCORE--- 0.7586017813655123 --actual-- ['a', 'man', 'climbs', 'a', 'mountain'] --pred-- ['a', 'man', 'is', 'rock', 'climbing']\n",
      "508929192_670910fdd2 ---HIGH BLEU SCORE--- 1.0 --actual-- ['children', 'are', 'playing', 'badminton', 'in', 'the', 'garden'] --pred-- ['two', 'children', 'playing', 'soccer']\n",
      "566794440_f9ec673a2f ---LOW BLEU SCORE--- 0.0 --actual-- ['girls', 'with', 'baseball', 'gloves', 'walking', 'across', 'a', 'baseball', 'field'] --pred-- ['three', 'men', 'play', 'hockey']\n",
      "693785581_68bec8312a ---HIGH BLEU SCORE--- 0.7221774642977133 --actual-- ['a', 'boy', 'does', 'a', 'flip', 'while', 'another', 'boy', 'stands', 'on', 'the', 'matt'] --pred-- ['two', 'children', 'are', 'playing', 'soccer']\n",
      "751737218_b89839a311 ---LOW BLEU SCORE--- 0.0 --actual-- ['a', 'boy', 'tries', 'to', 'throw', 'water', 'on', 'another', 'person', 'with', 'his', 'water', 'bottle'] --pred-- ['two', 'boys', 'are', 'playing', 'soccer']\n",
      "762947607_2001ee4c72 ---LOW BLEU SCORE--- 0.0 --actual-- ['a', 'boy', 'walks', 'in', 'a', 'meadow', 'near', 'some', 'trees'] --pred-- ['two', 'boys', 'playing', 'soccer']\n",
      "835415474_7b7f2a9768 ---LOW BLEU SCORE--- 0.0 --actual-- ['a', 'baby', 'hangs', 'off', 'an', 'adult', 's', 'back', 'while', 'laughing'] --pred-- ['two', 'men', 'in', 'red', 'shirts', 'play', 'basketball']\n",
      "837919879_94e3dacd83 ---LOW BLEU SCORE--- 0.0 --actual-- ['two', 'guys', 'are', 'playing', 'horse', 'shoe', 'together'] --pred-- ['a', 'boy', 'in', 'a', 'red', 'shirt', 'is', 'riding', 'a', 'bike', 'on', 'a', 'dirt', 'road']\n",
      "Average score is:  0.4210569449263458\n",
      "image_count: 1003\n",
      "-----------BEGIN COSINE SCORING----------------\n",
      "[[0.05227631]]\n",
      "Img: 407678652_1f475acd65 -- 0.052276313\n",
      "pred sent:  ['two', 'children', 'and', 'a', 'man', 'in', 'a', 'red', 'coat', 'and', 'a', 'blue', 'and', 'white', 'shirt', 'and', 'black', 'pants', 'is']\n",
      "ref_sent: ['the', 'hikers', 'descend', 'on', 'the', 'mountain']\n",
      "[[0.7461533]]\n",
      "Img: 445655284_c29e6d7323 -- 0.7461533\n",
      "pred sent:  ['a', 'black', 'dog', 'is', 'running', 'through', 'the', 'snow']\n",
      "ref_sent: ['the', 'black', 'and', 'brown', 'dog', 'is', 'running', 'through', 'the', 'snow']\n",
      "[[0.00575419]]\n",
      "Img: 538825260_a4a8784b75 -- 0.0057541905\n",
      "pred sent:  ['two', 'young', 'boys', 'with', 'brown', 'hair', 'blowing', 'bubbles']\n",
      "ref_sent: ['the', 'girl', 'is', 'taking', 'a', 'drink', 'from', 'a', 'water', 'fountain']\n",
      "[[-0.02384419]]\n",
      "Img: 566794440_f9ec673a2f -- -0.023844192\n",
      "pred sent:  ['three', 'men', 'play', 'hockey']\n",
      "ref_sent: ['the', 'girls', 'return', 'to', 'the', 'field', 'during', 'their', 'softball', 'game']\n",
      "[[0.78440225]]\n",
      "Img: 708860480_1a956ae0f7 -- 0.78440225\n",
      "pred sent:  ['a', 'boy', 'in', 'a', 'swimming', 'pool']\n",
      "ref_sent: ['a', 'young', 'boy', 'with', 'a', 'necklace', 'on', 'in', 'the', 'water']\n",
      "[[0.04441012]]\n",
      "Img: 751737218_b89839a311 -- 0.044410117\n",
      "pred sent:  ['two', 'boys', 'are', 'playing', 'soccer']\n",
      "ref_sent: ['the', 'laughing', 'boy', 'holds', 'a', 'water', 'bottle', 'while', 'a', 'lady', 'with', 'wet', 'pants', 'walks', 'nearby']\n",
      "[[0.04864852]]\n",
      "Img: 753285176_f21a2b984d -- 0.04864852\n",
      "pred sent:  ['two', 'boys', 'are', 'playing', 'soccer']\n",
      "ref_sent: ['a', 'woman', 'with', 'her', 'sweatshirt', 'tied', 'around', 'her', 'waist', 'holding', 'hands', 'with', 'a', 'young', 'girl', 'in', 'a', 'green', 'shirt', 'as', 'they', 'walk', 'up', 'a', 'grassy', 'hill']\n",
      "[[0.01942277]]\n",
      "Img: 762947607_2001ee4c72 -- 0.019422766\n",
      "pred sent:  ['two', 'boys', 'playing', 'soccer']\n",
      "ref_sent: ['little', 'boy', 'running', 'in', 'a', 'grassy', 'field', 'towards', 'a', 'tree']\n",
      "[[0.8032893]]\n",
      "Img: 799199774_142b1c3bb2 -- 0.8032893\n",
      "pred sent:  ['a', 'boy', 'in', 'a', 'swimming', 'pool']\n",
      "ref_sent: ['boy', 'playing', 'on', 'a', 'pink', 'raft', 'in', 'a', 'pool']\n",
      "[[0.05322987]]\n",
      "Img: 799431781_65dc312afc -- 0.05322987\n",
      "pred sent:  ['two', 'young', 'children', 'are', 'playing', 'on', 'a', 'swing']\n",
      "ref_sent: ['three', 'young', 'boys', 'playing', 'hear', 'no', 'evil', '', 'speak', 'no', 'evil', '', 'see', 'no', 'evil']\n",
      "[[-0.04360005]]\n",
      "Img: 835415474_7b7f2a9768 -- -0.043600045\n",
      "pred sent:  ['two', 'men', 'in', 'red', 'shirts', 'play', 'basketball']\n",
      "ref_sent: ['a', 'small', 'child', 'is', 'hanging', 'upside', 'down', 'from', 'an', 'adult', 's', 'shoulder']\n",
      "Count: 11\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "COMP5623M Coursework on Image Caption Generation\n",
    "\n",
    "\n",
    "python decoder.py\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from datasets import Flickr8k_Images, Flickr8k_Features, Flickr8k_Test\n",
    "from models import DecoderRNN, EncoderCNN\n",
    "from utils import *\n",
    "import utils\n",
    "from config import *\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# if false, train model; otherwise try loading model from checkpoint and evaluate\n",
    "EVAL = True\n",
    "\n",
    "\n",
    "# reconstruct the captions and vocab, just as in extract_features.py\n",
    "lines = read_lines(TOKEN_FILE_TRAIN)\n",
    "image_ids, cleaned_captions = parse_lines(lines)\n",
    "\n",
    "#vocab = build_vocab(cleaned_captions) # shilpa commented\n",
    "\n",
    "#=========BEGIN Code to load & biuilt vocab=========================================\n",
    "# As building vocab with unique words takes lot of time because of \"set\" functionality, once the vocab was built with 3.2K words, the words were written to a text file \n",
    "# and the words are loaded to vocab instead of building the vocab each time. \n",
    "with open(\"uniqwordlist.txt\", \"r\") as filestream:\n",
    "    for line in filestream:\n",
    "        vocab_words=line.split(\",\")\n",
    "print(len(vocab_words))\n",
    "\n",
    "vocab = Vocabulary()\n",
    "# add the token words\n",
    "vocab.add_word('<pad>')\n",
    "vocab.add_word('<start>')\n",
    "vocab.add_word('<end>')\n",
    "vocab.add_word('<unk>')\n",
    "\n",
    "# TODO add the rest of the words from the cleaned captions here\n",
    "# vocab.add_word('word')\n",
    "final_vocab = [vocab.add_word(word) for word in vocab_words]\n",
    "print(\"vocab len-\",vocab.__len__())\n",
    "#=========END Code to load & biuilt vocab=========================================\n",
    "\n",
    "\n",
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "\n",
    "# initialize the models and set the learning parameters\n",
    "decoder = DecoderRNN(EMBED_SIZE, HIDDEN_SIZE, len(vocab), NUM_LAYERS).to(device)\n",
    "\n",
    "\n",
    "if not EVAL:\n",
    "\n",
    "    # load the features saved from extract_features.py\n",
    "    features = torch.load('features.pt', map_location=device)\n",
    "    print(\"Loaded features:\", features.shape)\n",
    "\n",
    "    features = features.repeat_interleave(5, 0)\n",
    "    dataset_train = Flickr8k_Features(\n",
    "        image_ids=image_ids,\n",
    "        captions=cleaned_captions,\n",
    "        vocab=vocab,\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=64, # change as needed\n",
    "        shuffle=True,\n",
    "        num_workers=2, # may need to set to 0\n",
    "        collate_fn=caption_collate_fn, # explicitly overwrite the collate_fn\n",
    "    )\n",
    "\n",
    "\n",
    "    # loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(decoder.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#\n",
    "#        QUESTION 1.3 Training DecoderRNN\n",
    "# \n",
    "#########################################################################\n",
    "\n",
    "    # TODO write training loop on decoder here\n",
    "\n",
    "\n",
    "    # for each batch, prepare the targets using this function in utils.py\n",
    "    # targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "    num_epoc=5\n",
    "    print_at = 100\n",
    "    total_step = len(train_loader)\n",
    "    epoch_loss=0\n",
    "    for epoch in range(num_epoc):   \n",
    "        epoch_loss = 0       \n",
    "        for i, (feature, captions, lengths) in enumerate(train_loader):\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = decoder(feature, captions, lengths)\n",
    "            loss = criterion(outputs, targets)\n",
    "            decoder.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            if i % print_at == 0:\n",
    "                print('Epoch [{}/{}], print_at [{}/{}], Loss: {:.3f}'.format(epoch, num_epoc, i, total_step, loss.item())) \n",
    "\n",
    "    # save model after training\n",
    "    decoder_ckpt = torch.save(decoder, \"decoder.ckpt\")\n",
    "\n",
    "# if we already trained, and EVAL == True, reload saved model\n",
    "else:\n",
    "\n",
    "    data_transform = transforms.Compose([ \n",
    "        transforms.Resize(224),     \n",
    "        transforms.CenterCrop(224), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms\n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "\n",
    "    test_lines = read_lines(TOKEN_FILE_TEST)\n",
    "    test_image_ids, test_cleaned_captions = parse_lines(test_lines)\n",
    "\n",
    "    # load models\n",
    "    encoder = EncoderCNN().to(device)\n",
    "    decoder = torch.load(\"decoder.ckpt\").to(device)\n",
    "    encoder.eval()\n",
    "    decoder.eval() # generate caption, eval mode to not influence batchnorm\n",
    "\n",
    "    dataset_test = Flickr8k_Test(\n",
    "        image_ids=test_image_ids,\n",
    "        captions=test_cleaned_captions,\n",
    "        transform=data_transform\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    print(len(dataset_test))\n",
    "    print(len(test_loader))\n",
    "    print(len(test_image_ids))\n",
    "    \n",
    "\n",
    "#########################################################################\n",
    "#\n",
    "#        QUESTION 2.1 Generating predictions on test data\n",
    "# \n",
    "#########################################################################\n",
    "\n",
    "    # TODO define decode_caption() function in utils.py\n",
    "    # predicted_caption = decode_caption(word_ids, vocab)    \n",
    "    #def decode_caption(self, word_ids, vocab):\n",
    "    #    return predicted_caption\n",
    "    \n",
    "    \n",
    "    #------------ Newly defined method in util to create a json that holds actual and predicted captions against a imageid for whole testdata set ------------\n",
    "    all_ref_pred = create_reference_predicted_json()\n",
    "    \n",
    "    #------------ Below block of code does the prediction for the whole test dataset by called \"decode_caption\" method in util.py and stores the predction for each image in a json file called \"all_ref_pred\". Late this JSON is used to calculate BLEU score and Cosine similarity score on actual vs predicted captions ------------\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#\n",
    "#        QUESTION 2.2-3 Caption evaluation via text similarity \n",
    "# \n",
    "#########################################################################\n",
    "\n",
    "\n",
    "    # Feel free to add helper functions to utils.py as needed,\n",
    "    # documenting what they do in the code and in your report\n",
    "\n",
    "    \n",
    "    \n",
    "    ### Before BLEU scoring import the JSON which has actual captions & predicted captions \n",
    "    with open('prediction_for_BLEU_score.txt') as json_file:\n",
    "        all_ref_pred = json.load(json_file)\n",
    "    \n",
    "    \n",
    "    ### This method calculates the BLEU score.\n",
    "    ### Use the JSON file created that has actual and predicted captions to calculate average blue score.\n",
    "    ### Here 0.25 weight is used for each of 1-gram, 2-gram, 3-gram & 4-grams, smoothing function used is method4 \n",
    "    def calculate_bleu_score():\n",
    "        print(\"-----------BEGIN BLEU SCORING----------------\")\n",
    "        image_count=0\n",
    "        cumulative_score=0\n",
    "        s1=0\n",
    "        cc = SmoothingFunction()\n",
    "        for image in all_ref_pred:\n",
    "            if(\"predicted\" in all_ref_pred[image]):\n",
    "                image_count=image_count+1\n",
    "                reference = all_ref_pred[image][\"actualcaptions\"]\n",
    "                candidate = all_ref_pred[image][\"predicted\"][0]\n",
    "\n",
    "                cumulative_score+=sentence_bleu(reference,candidate,weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=cc.method4)\n",
    "                \n",
    "                # calc BLEU score using 0.25 weight for each gram & smoothing= method4\n",
    "                s1=sentence_bleu(reference,candidate,weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=cc.method4)\n",
    "                \n",
    "                bleu_min =0\n",
    "                bleu_max=1.6736959844303068\n",
    "                s1 = (s1 - bleu_min) / (bleu_max - bleu_min)\n",
    "                if(s1>0.7):\n",
    "                    print(image, \"---HIGH BLEU SCORE---\", s1 , \"--actual--\",reference[0], \"--pred--\",candidate)\n",
    "\n",
    "                if(  s1<0.08):\n",
    "                    print(image, \"---LOW BLEU SCORE---\", s1 , \"--actual--\",reference[0], \"--pred--\",candidate)\n",
    "                 \n",
    "                #if(s1>0.88 and s1<1):\n",
    "                #     print(image, \"-----\", s1 , \"--\", s2, \"--\", s3,\"--\", s4, \"--actual--\",reference[0], \"--pred--\",candidate)\n",
    "        print( \"Average score is: \" , cumulative_score/image_count)\n",
    "        print(\"image_count:\" ,image_count)\n",
    "        \n",
    "        \n",
    "    ### Invoke the method to print the average/high and low bleu score on the predictions    \n",
    "    calculate_bleu_score()    \n",
    "    \n",
    "    \n",
    "    ### This method finds the mean vector for each sentence. \n",
    "    ### The input passed to this method is a sentence which can be actual or predicted caption.\n",
    "    ### The id of each word in sentence is found and then embedding for each work is retreived.\n",
    "    ### Then the mean vector of all the words is calculated using torch.mean\n",
    "    ### The function returns a tensor of size 1,256\n",
    "    def find_sentence_mean(sentence):\n",
    "        embed_tensor = torch.tensor(())\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                idx = torch.tensor([vocab.word2idx[word]], dtype =torch.long)\n",
    "                embedding = decoder.embed(idx)\n",
    "                #print(\"embed_tensor shape:\", embedding.shape) (1,256)\n",
    "                embed_tensor = torch.cat([embed_tensor, embedding],0)\n",
    "                #print(\"embed_tensor shape:\",embed_tensor.shape) # (7,256) keeps adding\n",
    "            except:\n",
    "                pass        \n",
    "        #print(\"---embed_tensor shape:\",embed_tensor.shape) #torch.Size([7, 256])\n",
    "        #print(\"torch mean---\",torch.mean(embed_tensor, 0).shape) # torch.Size([256])\n",
    "        embed_tensor = torch.unsqueeze(torch.mean(embed_tensor, 0),0) #torch.Size([1, 256])\n",
    "        #print(\"---embed_tensor shape:\",embed_tensor.shape)\n",
    "        return embed_tensor    \n",
    "    \n",
    "    ### This method prints the average/high & low Cosine score on predicted image captions\n",
    "    def calculate_cosine_score():\n",
    "        print(\"-----------BEGIN COSINE SCORING----------------\")\n",
    "        count_image=0\n",
    "        count_score_match=0\n",
    "        for image in all_ref_pred:\n",
    "        #     if(count_image==10):\n",
    "        #         break\n",
    "            if(\"predicted\" in all_ref_pred[image]):\n",
    "                count_image=count_image+1\n",
    "                actual = all_ref_pred[image][\"actualcaptions\"]\n",
    "                pred_sent = all_ref_pred[image][\"predicted\"][0]\n",
    "                pred_mean=find_sentence_mean(pred_sent)\n",
    "                list_all_cos_sim = []\n",
    "                for ref_sent in actual:\n",
    "                    ref_mean = find_sentence_mean(ref_sent)\n",
    "                    cosine_sim_value = cosine_similarity(pred_mean.detach().numpy() ,ref_mean.detach().numpy() )\n",
    "                    #print(cosine_sim_value)\n",
    "                    list_all_cos_sim.append(cosine_sim_value)\n",
    "\n",
    "                avg = sum(list_all_cos_sim) / 5\n",
    "                if(avg[0][0] <0.06  or avg[0][0] >0.7):\n",
    "                    print(avg)\n",
    "                    #print(list_all_cos_sim)\n",
    "                    count_score_match=count_score_match+1\n",
    "                    print(\"Img:\",image,\"--\", avg[0][0])\n",
    "                    print(\"pred sent: \" , pred_sent)\n",
    "                    print(\"ref_sent:\", ref_sent)\n",
    "                #print(\"avg - \", sum(list_all_cos_sim) / 5) \n",
    "        print(\"Count:\",count_score_match)        \n",
    "        \n",
    "    ### Invoke the method to print the average/high and low Cosine score on the predictions    \n",
    "    calculate_cosine_score()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine\n",
    "0.80\n",
    "-0.04\n",
    "\n",
    "Bleu\n",
    "1.67\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00428577380952381"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg1=-0.04360005\n",
    "cos_min =-0.04\n",
    "cos_max=0.8\n",
    "normalized_cos_avg = (avg1 - cos_min) / (cos_max - cos_min)\n",
    "normalized_cos_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
